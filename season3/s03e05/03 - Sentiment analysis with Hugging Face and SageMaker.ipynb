{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Hugging Face and Amazon Sagemaker\n",
    "\n",
    "## Sentiment analysis on product reviews\n",
    "\n",
    "* https://huggingface.co/distilbert-base-uncased\n",
    "* https://huggingface.co/transformers/model_doc/distilbert.html\n",
    "* https://huggingface.co/datasets/generated_reviews_enth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip -q install awscli \"sagemaker>=2.31.0\" \"transformers>=4.5.0\" \"datasets[s3]==1.5.0\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install widgetsnbextension ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip -q install torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We are using the `datasets` library to download and preprocess the `imdb` dataset. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job. The [imdb](http://ai.stanford.edu/~amaas/data/sentiment/) dataset consists of 25000 training and 25000 testing highly polar movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = load_dataset('generated_reviews_enth', split=['train', 'validation', 'test'])\n",
    "\n",
    "print(train_dataset.shape)\n",
    "print(valid_dataset.shape)\n",
    "print(test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_stars_to_sentiment(row):\n",
    "    return {\n",
    "        'labels': 1 if row['review_star'] >= 4 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(map_stars_to_sentiment)\n",
    "valid_dataset = valid_dataset.map(map_stars_to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.flatten()\n",
    "valid_dataset = valid_dataset.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns(['correct', 'translation.th', 'review_star'])\n",
    "valid_dataset = valid_dataset.remove_columns(['correct', 'translation.th', 'review_star'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.rename_column('translation.en', 'text')\n",
    "valid_dataset = valid_dataset.rename_column('translation.en', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = valid_dataset.map(tokenize, batched=True, batch_size=len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dumps(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "s3_prefix = 'hugging-face/sentiment-analysis'\n",
    "\n",
    "train_input_path = f's3://{bucket}/{s3_prefix}/training'\n",
    "train_dataset.save_to_disk(train_input_path, fs=s3)\n",
    "\n",
    "valid_input_path = f's3://{bucket}/{s3_prefix}/validation'\n",
    "valid_dataset.save_to_disk(valid_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using the output from a SageMaker Processing job\n",
    "train_input_path = 's3://sagemaker-us-east-1-613904931467/sagemaker-scikit-learn-2021-04-12-17-18-37-118/output/training'\n",
    "valid_input_path = 's3://sagemaker-us-east-1-613904931467/sagemaker-scikit-learn-2021-04-12-17-18-37-118/output/validation'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_input_path)\n",
    "print(valid_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the Hugging Face model on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "    'epochs': 1,\n",
    "    'train_batch_size': 32,\n",
    "    'model_name':'distilbert-base-uncased'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "    role=role,\n",
    "    # Fine-tuning script\n",
    "    entry_point='train.py',\n",
    "    hyperparameters=hyperparameters,\n",
    "    # Infrastructure\n",
    "    transformers_version='4.4.2',\n",
    "    pytorch_version='1.6.0',\n",
    "    py_version='py36',\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count=1,\n",
    "    # Managed Spot Training\n",
    "    use_spot_instances=True,\n",
    "    max_wait=3600,\n",
    "    max_run=3600,\n",
    "    # Disable profiling\n",
    "    disable_profiler=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "huggingface_estimator.fit({'train': train_input_path, 'valid': valid_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve model, load it and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s $huggingface_estimator.model_data\n",
    "aws s3 cp $1 .\n",
    "mkdir -p model\n",
    "tar -xvzf model.tar.gz -C model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig, DistilBertForSequenceClassification\n",
    "\n",
    "config = AutoConfig.from_pretrained('./model/config.json')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('./model/pytorch_model.bin', config=config)\n",
    "\n",
    "print(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('This is fantastic product, best purchase in a long time!', return_tensors='pt')\n",
    "#inputs = tokenizer('What a rip-off, I want my money back', return_tensors='pt')\n",
    "\n",
    "print(inputs.input_ids)\n",
    "#print(inputs.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "    \n",
    "def top_class(logits):\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    print(softmax(logits))\n",
    "    pred = np.argmax(softmax(logits).detach().numpy(), axis=1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_class(outputs.logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the Hugging Face model on SageMaker with Data Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "    'epochs': 8,\n",
    "    'train_batch_size': 32,\n",
    "    'model_name':'distilbert-base-uncased'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(\n",
    "    role=role,\n",
    "    # Fine-tuning script\n",
    "    entry_point='train.py',\n",
    "    hyperparameters=hyperparameters,\n",
    "    # Infrastructure\n",
    "    transformers_version='4.4.2',\n",
    "    pytorch_version='1.6.0',\n",
    "    py_version='py36',\n",
    "    instance_type='ml.p3.16xlarge',\n",
    "    instance_count=2,\n",
    "    # Managed Spot Training\n",
    "    use_spot_instances=True,\n",
    "    max_wait=3600,\n",
    "    max_run=3600,\n",
    "    # Data Parallelism\n",
    "    distribution={'smdistributed': {'dataparallel': {'enabled': True}}}\n",
    ")\n",
    "\n",
    "huggingface_estimator.fit({'train': train_input_path, 'valid': valid_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the Hugging Face model on SageMaker with Model Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "    'epochs': 1,\n",
    "    'train_batch_size': 32,\n",
    "    'model_name':'distilbert-base-uncased'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpi_options = {\n",
    "    \"enabled\" : True,\n",
    "    \"processes_per_host\" : 2\n",
    "}\n",
    "\n",
    "smp_options = {\n",
    "    \"enabled\": True,\n",
    "    \"parameters\": {\n",
    "        \"microbatches\": 2,\n",
    "        \"placement_strategy\": \"spread\",\n",
    "        \"pipeline\": \"interleaved\",\n",
    "        \"optimize\": \"memory\",\n",
    "        \"partitions\": 4\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(\n",
    "    role=role,\n",
    "    # Fine-tuning script\n",
    "    entry_point='train.py',\n",
    "    hyperparameters=hyperparameters,\n",
    "    # Infrastructure\n",
    "    transformers_version='4.4.2',\n",
    "    pytorch_version='1.6.0',\n",
    "    py_version='py36',\n",
    "    instance_type='ml.p3dn.24xlarge',    # 8 NVIDIA V100 GPUs with 32GB memory = 256GB \n",
    "    instance_count=1,                    \n",
    "    # Managed Spot Training\n",
    "    use_spot_instances=True,\n",
    "    max_wait=3600,\n",
    "    max_run=3600,\n",
    "    # Data Parallelism\n",
    "    distribution={\"smdistributed\": {\"modelparallel\": smp_options}, \"mpi\": mpi_options}\n",
    ")\n",
    "\n",
    "huggingface_estimator.fit({'train': train_input_path, 'valid': valid_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
